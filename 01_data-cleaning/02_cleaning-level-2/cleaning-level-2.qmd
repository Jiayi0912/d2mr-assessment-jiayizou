---
title: 'Level 2 Data Cleaning: Clean the midwest Dataset'
---

# Objective

The objective of this assignment is to practice cleaning and transforming a messy dataset using tidyverse functions. You will use skills like renaming and reordering columns, sorting rows, changing data types, mutating data, and using the stringr and forcats packages.

This is the Level 2 Data Cleaning assignment. You may additionally or alternatively complete the [Level 1 Data Cleaning assignment](https://github.com/nrdowling/d2mr-assessment/tree/main/01_data-cleaning/01_cleaning-level-1), in which you will work with a simple dataset and focus on basic data cleaning tasks. The Level 1 assignment has more direct instruction and is recommended for those who are new to data cleaning.

In this Level 2 Cleaning assignment, you will work with a more complex dataset and perform additional cleaning tasks with less direct instruction. The Level 2 assignment has more opportunities to demonstrating meeting course standards than this Level 1 assignment and is recommended for those who are already comfortable with the tasks in this assignment.

# Instructions

1. If you have not already done so, pull the latest changes from the `d2mr-assessment` repository to ensure you have the most up-to-date version of the assignment files. Confirm you are working in your fork of the repository.
2. Open `cleaning-level-2.qmd` in RStudio and follow the instructions in the Setup section below to load and inspect the (original) `midwest` dataset. 
    - **Important:** Unlike Level 1, you will not be provided with a goal dataset to match. Instead, you will evaluate what cleaning tasks are necessary or useful *in principle*. You can reference the original `midwest` dataset, but ultimately you will need to decide what the "clean" version of the dataset should look like.
3. Follow the guideline to identify and perform cleaning tasks on the `messy-midwest.csv` dataset.
4. At some points in this document you may come across questions or non-coding exercises. Answer these questions in the text of this .qmd document, immediately below the question.
5. *Optional:* Continue to follow the instructions in the assignment script to clean the dataset above and beyond matching the original. 

# Setup

Run these chunks as written. Do not make changes to code except where noted if necessary.

## Loading libraries and set seed

```{r}
#| label: setup
library(tidyverse)
set.seed(1234)
```


## Read in and inspect messy data

Read in and inspect the messy dataset `messy-midwest.csv`.

```{r}

#| label: read-messy-data

### LEAVE THIS CHUNK AS-IS ###

# You *might* need to edit the filepath, but don't change anything else!

# Read in messy-midwest.csv
messy.midwest <- read_csv(
  ########################################
  "./messy-midwest.csv", ## <-- THIS IS THE ONLY THING IN THIS CHUNK YOU CAN CHANGE IF NECESSARY
  ########################################
  trim_ws = FALSE, name_repair = "minimal", col_types = cols(.default = col_character()))

# Inspect the structure and contents of the messy midwest dataset with head(), glimpse(), str(), and/or View()
head(messy.midwest)
glimpse(messy.midwest)
str(messy.midwest)
View(messy.midwest)

```

## Inspect the original midwest dataset

```{r}
#| label: inspect-original-data

### LEAVE THIS CHUNK AS-IS ###

# Load the original midwest dataset
data(midwest)

# View the documentation for the midwest dataset
?midwest

# Inspect the structure and contents original midwest dataset with head(), glimpse(), str(), and/or View()
head(midwest)
glimpse(midwest)
str(midwest)
View(midwest)

```

QUESTIONS:

1. What are the differences between the messy dataset and the original midwest dataset?

<!-- answer below -->
```{r include=FALSE}
#| label: compare-datasets
# Use all.equal to compare the messy and original midwest datasets
all.equal(messy.midwest, midwest)

# using daff to compare
library(daff)

diff_data(messy.midwest, midwest)
render_diff(diff_data(messy.midwest, midwest))
```
Using the `all.equal` ang `daff` function, we can see a total of `r length(all.equal(messy.midwest, midwest))` differences between the two datasets. Specifically, compared to the original dataset, in the messy midwest dataset:
- The messy dataset has two missing variables (percblack and percwhite) and a duplicate variable (Population per Sq Mile, which is equal to Population Density)
- Column order is different
- 21 column names are messy, including problems of capitalization, extra spaces, typos, and adding units
- Data types are all stored as character, while many of them are numeric in the original dataset
- Values in the County and State column have lots of problems (extra spaces, capitalization, typos, etc)
- Data in the first 27 columns mismatches
- Numbers aren't rounded to the same number of decimal places

2. What are the biggest issues you need to address in cleaning?

<!-- answer below -->
- Deleting the duplicate column and adding the missing columns
- Rename columns
- Reorder columns
- Change data types: except for County, change State and Category columns to factor, and other columns to numeric
- Correct values in County and State columns: remove extra spaces, fix typos, ensure consistent capitalization
- Round numbers to the same number of decimal places

3. Are there any differences between the messy dataset and the original dataset that you do not need or want to address in cleaning? If so, why (not)?

<!-- answer below -->
- Replacing data in "Population Density" with the ones in "Population per Sq Mile" is better than preserving the original data, since the latter uses consistent calculation unit and reduces misunderstanding.
- Changing the State data type to factor instead of character is more useful when generating regression or ggplot. Since the column only contains 5 variables ("IL", "IN", "MI", "OH", "WI"), converting character to factor helps to analyse data by state/area.
- For different numeric variables, the number of decimal places is also different. For variables that stored percentage of certain population, round numbers to 3 decimal places is enough to preserve accuracy of the data. For population density, rounding numbers to 1 decimal place makes sense statistically.


4. Are there additional cleaning tasks you would like to perform beyond matching the original dataset? If so, what are they and why do you think they are important?

<!-- answer below -->

# Cleaning

You may approach cleaning the dataset however you like based on how you identified problems above and how you think they should be prioritized.

If you're not sure where to start, you can organize your cleaning into the following categories. **You do not need to follow this structure.** Feel free to delete these sections, rearrange them, or add new ones as needed. (FYI: When I cleaned this myself I loosely followed this structure, but there were some parts of my approach that could not have worked in this order.)

You can additionally/alternatively construct your cleaning in a single pipeline in the last chunk.

## Creating a cleaning dataset
```{r}

#| label: make-cleaning-dataset

# Create a dataset to work with during the cleaning process called "clean.midwest"
clean.midwest <- messy.midwest

```


## Selecting, renaming, and reordering columns

```{r}
#| label: rename-columns

## Rename columns to match the original iris dataset using the select() function
## Put them in the order of the original dataset
clean.midwest <- messy.midwest %>%
  select(PID, county = `C0unty Name`, state = State, area_sqm = `Area (sq miles)`, 
         poptotal = `Total P0pulation`, popdensity_persqm = `Population per Sq Mile`, popwhite, 
         popblack, popamerindian, popasian, popother,percamerindian = percamerindan, 
         percasian = `Percentage Asian`, percother, popadults = `Population Over 18`, perchsd, 
         percollege = `Percentage College`, percprof, poppovertyknown, percpovertyknown, 
         percbelowpoverty = `Percentage Below Poverty`, percchildpoverty = percchildbelowpovert, 
         percadultpoverty, percelderlypoverty, inmetro, category)

# In this way, the duplicate column "Population Density" is deleted, because we want to preserve data in "Population per Sq Mile"
# Current column order still misses two variables: percwhite and percblack

```


```{r}

#| label: checkpoint-1-allequal

# Inspect the current state of the dataset
head(clean.midwest)

# Use all.equal() to check if clean.midwest matches midwest
all.equal(clean.midwest, midwest)

```
Now we've got `r length(all.equal(clean.midwest, midwest))` flagged differences. 
- The biggest difference now is still the data types.
- We should also notice the remaining problems above: two variables percwhite and percblack haven't been created yet, and they need to be inserted between popother and percamerindian to put the columns in order. But this step can only be done after we after we change data types.

## Changing data types and cleaning values

### Cleaning character variables: county, state, and category

#### Cleaning county: clean values
We start with tackling the character variables first. In question 2, we have decided to change State and Category variables into factor, and preserve County as a character variable.

```{r}
#| label: unique-county-1

# Check the unique values of the County column in clean.midwest
unique(clean.midwest$county)

```
Using the unique() function, we can see the data contains 3 types of error in general:
- Extra spaces, both at the beginnings and ends of the values
- Typos: "0" instead of letter "o", "1" instead of letter "l", and "3" instead of letter "e"
- All values are in all lowercase, while everything is lower case in `midwest$county`

We can primarily clean the data with these 3 patterns.

```{r}

#| label: clean-county-patterns

# Use stringr to clean the county column
clean.midwest <- clean.midwest %>%
    mutate(
        # Trim whitespace from the beginning and end of the string
        county = str_squish(county), 
        # Replace 0 with o, 1 with l, 3 with e
        county = str_replace_all(county, c("0" = "o", "1" = "l", "3" = "e")),
        # Convert to uppercase
        county = str_to_upper(county),
    )
```

Now we can find the unique values that's left in the County column.

```{r}
#| label: unique-county-2

# Use setdiff() to check the unique values of the County column in clean.midwest
setdiff(clean.midwest$county, midwest$county)
```
No problems left in the County column! Fortunately it is already a character variable, so no further procedure is needed for now.

In the next step we can deal with the State column. For State and Category column that we want to change from character to factor, we need to first clean the data value and then convert the data type.

```{r}
#| label: unique-state-1

# Check the unique values of the State column in clean.midwest
unique(clean.midwest$state)

```
We use the unique() function to check with value errors again, and find generally 3 issues:
- Several values contain periods at the end of values
- Inconsistent name of the same state. "Wis." and "Wisconsin" should be "WI",  "Mich." and "Michigan" should be "MI", "OHIO" and "Ohio" should be "OH", "Ind." and "Indiana" should be "IN", and "Ill." and "Illinois" should be "IL"
- Inconsistent capitalization (all values are uppercase in midwest)

These 3 problems can be solved at the same time if we directly substitute the error state name with the correct ones. Now we can conduct data cleaning on the State column.

```{r}

#| label: clean-state-patterns

# Use stringr to clean the county column
clean.midwest <- clean.midwest %>%
    mutate(
        # Replace "Wis." and "Wisconsin" with "WI" (replace "Wisconsin" because it's longer)
        state = str_replace_all(state, "Wisconsin|Wis\\.", "WI"),
        # Replace "Mich." and "Michigan" with "MI"
        state = str_replace_all(state, "Michigan|Mich\\.", "MI"),
        # Replace "OHIO" and "Ohio" with "OH"
        state = str_replace_all(state, "OHIO|Ohio", "OH"),
        # Replace "Ind." and "Indiana" with "IN"
        state = str_replace_all(state, "Indiana|Ind\\.", "IN"),
        # Replace "Ill." and "Illinois" with "IL"
        state = str_replace_all(state, "Illinois|Ill\\.", "IL")
    )
```

Now let's check the unique values of state again:
```{r}
#| label: unique-state-2

unique(clean.midwest$state)

```
The State column values are ideal now. We can convert this column into a factor for later data analysis. After converting data type, state becomes a factor variable with 5 levels: "IL", "IN", "MI", "OH", and "WI".

```{r}

#| label: factor-state

# Convert the State column to a factor
clean.midwest <- clean.midwest %>%
    mutate(state = factor(state))

levels(clean.midwest$state)

```

Lastly, we can examine the unique values in the Category column.

```{r}
#| label: unique-category-1

# Check the unique values of the Category column in clean.midwest
unique(clean.midwest$category)

```
The result indicates that there are no errors in column values. We can simply convert it to a factor with 10 levels: "AAR" "AAU" "AHR" "AHU" "ALR" "ALU" "HAR" "HAU" "HHR" "HHU" "HLR" "HLU" "LAR" "LAU" "LHR" "LHU"

```{r}

#| label: factor-category

# Convert the Category column to a factor
clean.midwest <- clean.midwest %>%
    mutate(category = factor(category))

levels(clean.midwest$category)

```
Finally, after dealing with these three originally character variables, county, state, and category, we can once again check whether clean.midwest identifies with midwest.

```{r}

#| label: checkpoint-2-allequal

# Inspect the current state of the dataset
head(clean.midwest)

# Use all.equal() to check if clean.midwest matches midwest
all.equal(clean.midwest, midwest)

```

We can begin tackling with the rest variables containing numbers.

## Cleaning numeric variables: area_sqm, popdensity_persqm, and perc~

## Mutating data

```{r}

```


## Using stringr and forcats

```{r}

```


## Other cleaning tasks

```{r}

```




## All cleaning in a single pipeline


```{r}

#| label: one-pipeline


```



# Reflection

QUESTIONS:

1. Is your dataset identical to `midwest`? If not, what is different? (Remember the functions `all.equal()` and `diff_data()` can help here.)

<!-- answer below -->

2. Did you make any choices to clean the data that resulted in the dataset not matching the original? Why did you make those choices?

<!-- answer below -->

3. Were there any cleaning steps -- whether necessary to recreate the original df or just because you wanted to do them -- that you weren't able to implement? If so, what were they and what more would you need to do/know to implement them? 

<!-- answer below -->


# Unguided cleaning and transformation

*Optional:* If you have the time and interest, continue transforming this dataset as you please. Create new columns based on the existing ones, reformat strings, try your hand at a regex replacement, summarize by groups (factor levels), visualize a simple relationship, or anything else you can think of. To get you started, consider things like:

1. **Exploratory Data Analysis:** Use the cleaned dataset to explore relationships between variables, create visualizations, and generate insights.
2. **Data Transformation:** Create new variables, aggregate data, or reshape the dataset to prepare it for analysis.
3. **Split, Merge, and Reshape:** Split the dataset into multiple datasets or merge it with other datasets using `join` functions to create a new dataset. Use `pivot_longer()` and `pivot_wider()` to reshape the data.
4. **Informativity:** Consider the midwest data and its documentation. Clean/transform the dataframe into a format that is more informative, transparent, or easier to work with. For example, improve column naming conventions, create new (useful) variables, reduce redundancy, or eliminate variables that are not useful or documented.


# Submission & Assessment

To submit:

1. Add an `assessment.md` file to this mini-project's directory:
    1. Check off all objectives you believe you have demonstrated
    2. Indicate which unique objectives you are meeting for the first time (if any)
    3. Complete any relevant open-ended items
2. Push your changes to your centralized assignment repository on GitHub. 
3. Confirm that Dr. Dowling and your section TA are added as collaborators to your repository.
4. Submit your work in your next open mini-project assignment by including the following information in the text box:
    1. The title of the assignment: "Level 2 Data Cleaning: Clean the midwest Dataset"
    2. A link to the **directory** for this assignment in your centralized assignment repo



